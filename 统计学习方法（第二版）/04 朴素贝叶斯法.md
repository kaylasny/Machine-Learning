---
layout: post
mathjax: true
---

# 朴素贝叶斯法

朴素贝叶斯（naïve Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。

设输入空间 $\mathcal{X} \subseteq \mathbb{R}^n$ 为 $n$ 维向量的集合，输出空间为类标记集合 $\mathcal{Y} = \{c_1, c_2, \cdots, c_K\}$。输入为特征向量 $x \in \mathcal{X}$，输出为类标记（class label）$y \in \mathcal{Y}$。$X$ 是定义在输入空间 $\mathcal{X}$ 上的随机向量，$Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。$P(X, Y)$ 是 $X$ 和 $Y$ 的联合概率分布。训练数据集

$$T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$$

由 $P(X, Y)$ 独立同分布产生。

<span style="color:red;"><b>先验概率分布：</b></span>

$P(Y = c_k), \quad k = 1, 2, \dots, K$

<span style="color:red;"><b>条件概率分布：</b></span>

$P(X = x \mid Y = c_k) = P(X^{(1)} = x^{(1)}, \dots, X^{(n)} = x^{(n)} \mid Y = c_k), \quad k = 1, 2, \dots, K$

$P(X = x \mid Y = c_k)$ 有指数级数量的参数，其估计实际是不可行的，因而朴素贝叶斯法对条件概率分布作了条件独立性的假设（用于分类的特征在类确定的条件下都是条件独立的）。

$P(X = x \mid Y = c_k) = P(X^{(1)} = x^{(1)}, \dots, X^{(n)} = x^{(n)} \mid Y = c_k) = \prod_{j=1}^{n} P(X^{(j)} = x^{(j)} \mid Y = c_k)$

<span style="color:red;"><b>后验概率分布：</b></span>

$P(Y = c_k \mid X = x) = \frac{P(X = x \mid Y = c_k) P(Y = c_k)}{P(X=x)} = \frac{P(X = x \mid Y = c_k) P(Y = c_k)}{\sum_k P(X = x \mid Y = c_k) P(Y = c_k)}$

由于后验概率分布中分母$P(X=x)$对所有$c_k$是相同的，因此<span style="color:red;"><b>朴素贝叶斯分类器</b></span>可表示为：

$$y = \arg \max_{c_k} P(Y = c_k) \prod_j P(X^{(j)} = x^{(j)} \mid Y = c_k)$$

> ### 算法 4.1 （朴素贝叶斯算法（naïve Bayes algorithm））
> 
> **输入**：训练数据 $T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$，其中 $x_i = (x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})^\top$，$x_i^{(j)} \in \{a_{j1}, a_{j2}, \cdots, a_{jS_j}\}$，$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值，$j = 1, 2, \cdots, n, l = 1, 2, \cdots, S_j, y_i \in \{c_1, c_2, \cdots, c_K\}$；实例 $x$；
> 
> **输出**：实例 $x$ 的分类。
> 
> #### (1) 计算先验概率及条件概率
> 
> $$
> P(Y = c_k) = \frac{\sum_{i=1}^N I(y_i = c_k)}{N}, \quad k = 1, 2, \cdots, K
> $$
> 
> $$
> P(X^{(j)} = a_{jl} \mid Y = c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl}, y_i = c_k)}{\sum_{i=1}^N I(y_i = c_k)}, 
> $$
> 
> 其中：
> 
> $$
> j = 1, 2, \cdots, n; \quad l = 1, 2, \cdots, S_j; \quad k = 1, 2, \cdots, K.
> $$
> 
> #### (2) 对于给定的实例 $x = (x^{(1)}, x^{(2)}, \cdots, x^{(n)})^\top$，计算
> 
> $$
> P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k), \quad k = 1, 2, \cdots, K.
> $$
> 
> #### (3) 确定实例 $x$ 的类
> 
> $$
> y = \arg\max_{c_k} P(Y = c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} \mid Y = c_k).
> $$

<span style="color:purple;">$\color{purple}{P(X^{(j)} = a_{jl} \mid Y = c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl}, y_i = c_k)}{\sum_{i=1}^N I(y_i = c_k)}}$ 计算出所有的情况，在处理 $\color{purple}{\prod_{j=1}^{n} P(X^{(j)} = x^{(j)} \mid Y = c_k)}$ 时要根据实际输入的 $x$ 从所有情况中取计算所需的值。</span>

极大似然估计可能会出现所要估计的值为 $0$ 的情况，这回影响到后验概率的计算结果，使分类产生偏差，因而可以使用贝叶斯估计：

- <span style="color:red;"><b>先验概率的贝叶斯估计：</b></span>$P_{\lambda}(Y = c_k) = \frac{\sum_{i=1}^N I(y_i = c_k) + \lambda}{N + K\lambda}$

- <span style="color:red;"><b>条件概率的贝叶斯估计：</b></span>$P_{\lambda}(X^{(j)} = a_{jl} \mid Y = c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl}, y_i = c_k) + \lambda}{\sum_{i=1}^N I(y_i = c_k) + S_j \lambda}$（$\lambda=1$称为拉普拉斯平滑）

可将两者回代上面朴素贝叶斯算法（截图）中的 $P(Y = c_k)$ 和 $P(X^{(j)} = a_{jl} \mid Y = c_k)$。




