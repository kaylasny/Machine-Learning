# 决策树

> **定义 5.1（决策树）**  
> 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

假设给定训练数据集：

$$
D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}
$$

其中，$x_i = (x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})^T$ 为输入实例（特征向量），$n$ 为特征个数，$y_i \in \{1, 2, \cdots, K\}$ 为类标记，$i = 1, 2, \cdots, N$，$N$ 为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

- 决策树学习用损失函数表示这一目标，决策树学习的策略是**以损失函数为目标函数的最小化**。

- 决策树学习通常包括三个步骤：<strong style="color:red">特征选取、决策树的生成和决策树的修剪</strong> 。

- 决策树学习本质上是从训练数据集中归纳出一组分类规则，希望获得一个与训练数据矛盾较小同时具有很好的泛化能力。

- 针对过拟合现象，需要对已经生成的树自下而上进行剪枝，让树变得简单，从而使它有更好的泛化能力。

- 如果特征数量很多，可以在决策树学习开始的时候对特征进行选择，只留下对训练数据有足够分类能力的特征。



## 特征选取

<strong style="color:red">信息增益</strong>

> **定义 5.2 （信息增益）**  
> 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即：
>
> $$
> g(D, A) = H(D) - H(D|A)
> $$



> **算法 5.1 （信息增益的算法）**  
> 输入：训练数据集 $D$ 和特征 $A$；  
> 输出：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$。  
> 
> 1. **计算数据集 $D$ 的经验熵 $H(D)$**  
> 
>    $$
>    H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}
>    $$
> 
> 2. **计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$**  
> 
>    $$
>    H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}
>    $$
> 
> 3. **计算信息增益**  
> 
>    $$
>    g(D, A) = H(D) - H(D|A)
>    $$

<strong style="color:red">信息增益比</strong>

> **定义 5.3 （信息增益比）**  
> 特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A)$ 定义为其信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即：
> 
> $$
> g_R(D, A) = \frac{g(D, A)}{H_A(D)} 
> $$
> 
> 其中，
> 
> $$
> H_A(D) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
> $$
> 
> $n$ 是特征 $A$ 取值的个数。



## 决策树的生成

<strong style="color:red">ID3 算法</strong>

> **算法 5.2 （ID3 算法）**  
> 输入：训练数据集 $D$，特征集 $A$ 阈值 $\varepsilon$；  
> 输出：决策树 $T$。  
> 
> 1. 若 $D$ 中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 2. 若 $A = \emptyset$，则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 3. 否则，按算法 5.1 计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_g$；  
> 4. 如果 $A_g$ 的信息增益小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；  
> 6. 对第 $i$ 个子结点，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归地调用步骤 (1)～(5)，得到子树 $T_i$，返回 $T_i$。

ID3 算法中只有树的生成，容易导致过拟合。



<strong style="color:red">C4.5 算法</strong>

> **算法 5.3 （C4.5 的生成算法）**  
> 输入：训练数据集 $D$，特征集 $A$ 阈值 $\varepsilon$；  
> 输出：决策树 $T$。  
>
> 1. 如果 $D$ 中所有实例属于同一类 $C_k$，则置 $T$ 为单结点树，并将 $C_k$ 作为该结点的类，返回 $T$；  
> 2. 如果 $A = \emptyset$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；  
> 3. 否则，按式 (5.10) 计算 $A$ 中各特征对 $D$ 的信息增益比，选择信息增益比最大的特征 $A_g$；  
> 4. 如果 $A_g$ 的信息增益比小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；  
> 5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；  
> 6. 对结点 $i$，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归地调用步骤 (1)～(5)，得到子树 $T_i$，返回 $T_i$。

C4.5 算法与 ID3 算法相似，在生成过程中使用信息增益比来选择特征。



## 决策树的剪枝

设树 $T$ 的叶结点个数为 $|T|$，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $N_{tk}$ 个，$k=1, 2, \cdots, K$，$H_t(T)$ 为叶结点 $t$ 上的经验熵，$\alpha \geq 0$ 为参数，则决策树学习的损失函数可以定义为：

$$
C_\alpha(T) = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T|
\tag{5.11}
$$

其中经验熵为：

$$
H_t(T) = -\sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}
\tag{5.12}
$$

在损失函数中，将式 (5.11) 右端的第 1 项记作：

$$
C(T) = \sum_{t=1}^{|T|} N_t H_t(T) = -\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{tk} \log \frac{N_{tk}}{N_t}
\tag{5.13}
$$

这时有：

$$
C_\alpha(T) = C(T) + \alpha |T|
\tag{5.14}
$$

式 (5.14) 中，$C(T)$ 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$ 表示模型复杂度，参数 $\alpha \geq 0$ 控制两者之间的影响。较大的 $\alpha$ 倾向于选择较简单的模型（树），较小的 $\alpha$ 倾向于选择较复杂的模型（树）。$\alpha=0$ 意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

剪枝，就是当 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 $\alpha$ 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。

> **算法 5.4（树的剪枝算法）**  
> 输入：生成算法产生的整个树 $T$，参数 $\alpha$；  
> 输出：修剪后的子树 $T_\alpha$。
> 1. 计算每个结点的经验熵。  
> 2. 递归地从树的叶结点向上回缩。  
>    - 设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_B$ 与 $T_A$，其对应的损失函数值分别是 $C_\alpha(T_B)$ 与 $C_\alpha(T_A)$，如果
>      $$
>      C_\alpha(T_A) \leq C_\alpha(T_B)
>      $$
>      则进行剪枝，即将父结点变为新的叶结点。
> 3. 返回 (2)，直至不能继续为止，得到损失函数最小的子树 $T_\alpha$。



## CART 算法

CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分。

CART 算法由以下两步组成：

1. **决策树生成**：基于训练数据集生成决策树，生成的决策树要尽量大；
2. **决策树剪枝**：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

### CART 生成

#### 2. 分类树的生成

> **定义 5.4 (基尼指数)** 
> 分类问题中，假设有 \( K \) 个类，样本点属于第 \( k \) 类的概率为 \( p_k \)，则概率分布的基尼指数定义为
>
> $$
> \text{Gini}(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2 \tag{5.22}
> $$
>
> 对于二类分类问题，若样本点属于第 1 个类的概率是 \( p \)，则概率分布的基尼指数为
> 
> $$
> \text{Gini}(p) = 2p(1-p) \tag{5.23}
> $$
> 
> 对于给定的样本集合 \( D \)，其基尼指数为
> 
> $$
> \text{Gini}(D) = 1 - \sum_{k=1}^K \left(\frac{|C_k|}{|D|}\right)^2 \tag{5.24}
> $$
> 
> 这里，\( C_k \) 是 \( D \) 中属于第 \( k \) 类的样本子集，\( K \) 是类的个数。

如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分，即

$$
D_1 = \{(x, y) \in D \mid A(x) = a\}, \quad D_2 = D - D_1
$$

则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为

$$
\text{Gini}(D, A) = \frac{|D_1|}{|D|} \text{Gini}(D_1) + \frac{|D_2|}{|D|} \text{Gini}(D_2) \tag{5.25}
$$

> **算法 5.6 (CART 生成算法)**
> 
> **输入**: 训练数据集 $D$，停止计算的条件  
> **输出**: CART 决策树  
> 
> 根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：
> 
> 1. 设结点的训练数据集为 $D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A$，对其可能取的每个值 $a$，根据样本点对 $A = a$ 的测试为“是”或“否”将 $D$ 分割成 $D_1$ 和 $D_2$ 两部分，利用公式 (5.25) 计算 $A = a$ 时的基尼指数。
> 
> 2. 在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。
> 
> 3. 对两个子结点递归地调用 (1)、(2)，直至满足停止条件。
> 
> 4. 生成 CART 决策树。


### CART 剪枝

<font color="purple">

Breiman 等人证明：可以用递归的方法对树进行剪枝。将 $\alpha$ 从小增大，$0 = \alpha_0 < \alpha_1 < \cdots < \alpha_n < +\infty$，产生一系列的区间 $[\alpha_i, \alpha_{i+1})$，$i = 0, 1, \cdots, n$；剪枝得到的子树序列对应着区间 $\alpha \in [\alpha_i, \alpha_{i+1})$，$i = 0, 1, \cdots, n$ 的最优子树序列 $\{T_0, T_1, \cdots, T_n\}$，序列中的子树是嵌套的。

</font>

#### 1. 剪枝，形成一个子树序列

从整体树 $T_0$ 开始剪枝。对 $T_0$ 的任意内部结点 $t$，以 $t$ 为单结点树的损失函数是

$$
C_\alpha(t) = C(t) + \alpha \tag{5.27}
$$

以 $t$ 为根结点的子树 $T_t$ 的损失函数是

$$
C_\alpha(T_t) = C(T_t) + \alpha |T_t| \tag{5.28}
$$

当 $\alpha = 0$ 且 $t$ 充分小时，有不等式

$$
C_\alpha(T_t) < C_\alpha(t) \tag{5.29}
$$

当 $\alpha$ 增大时，在某一 $\alpha$ 有

$$
C_\alpha(T_t) = C_\alpha(t) \tag{5.30}
$$

当 $\alpha$ 再增大时，不等式 (5.29) 反向。只要

$$
\alpha = \frac{C(t) - C(T_t)}{|T_t| - 1},
$$

$T_t$ 与 $t$ 有相同的损失函数值，而 $t$ 的结点少，因此比 $T_t$ 更可取，对 $T_t$ 进行剪枝。为此，对 $T_0$ 中每一内部结点 $t$，计算

$$
g(t) = \frac{C(t) - C(T_t)}{|T_t| - 1} \tag{5.31}
$$

它表示剪枝后整体损失函数减少的程度。在 $T_0$ 中剪去 $g(t)$ 最小的 $T_t$，将得到的子树作为 $T_1$，同时将最小的 $g(t)$ 设为 $\alpha_1$。$T_1$ 为区间 $[\alpha_1, \alpha_2)$ 的最优子树。

#### 2. 通过交叉验证选取最优子树

利用独立的验证数据集，测试子树序列 $T_0, T_1, \cdots, T_n$ 中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树 $T_1, T_2, \cdots, T_n$ 都对应于一个参数 $\alpha_1, \alpha_2, \cdots, \alpha_n$。所以，当最优子树 $T_k$ 确定时，对应的 $\alpha_k$ 也确定了，即得到最优决策树 $T_\alpha$。

> **算法 5.7 (CART 剪枝算法)**
>
> **输入**: CART 算法生成的决策树 $T_0$  
> **输出**: 最优决策树 $T_\alpha$  
>
> 1. 设 $k = 0, T = T_0$。
> 2. 设 $\alpha = +\infty$。
> 3. 自下而上地对各内部结点 $t$ 计算 $C(T_t)$、$|T_t|$ 以及
> 
> $$
> g(t) = \frac{C(t) - C(T_t)}{|T_t| - 1}, \quad \alpha = \min(\alpha, g(t))
> $$
>
>    这里，$T_t$ 表示以 $t$ 为根结点的子树，$C(T_t)$ 是对训练数据的预测误差，$|T_t|$ 是 $T_t$ 的叶结点个数。
>
> 4. 对 $g(t) = \alpha$ 的内部结点 $t$ 进行剪枝，并对叶结点 $t$ 以多数表决法决定其类，得到树 $T$。
> 5. 设 $k = k + 1, \alpha_k = \alpha, T_k = T$。
> 6. 如果 $T_k$ 不是由根结点及两个叶结点构成的树，则回到步骤 (2)；否则令 $T_k = T_n$。
> 7. 采用交叉验证法在子树序列 $T_0, T_1, \cdots, T_n$ 中选取最优子树 $T_\alpha$。


<font color="purple">

小结：

分类决策树模型是一种描述对实例进行分类的树形结构。

本章讨论的核心：特征选取、决策树的生成和决策树的修剪。

- 特征选取介绍了信息增益、信息增益比、基尼指数三个指标。
- 决策树的生成介绍了 “贪心 + 递归” 的选取策略。（如 ID3 算法和 C4.5 算法，只是特征选取的指标不同，其它是一样的）。
- 决策树的修建介绍了 $C_\alpha(T) = C(T) + \alpha |T|$ 评价指标。

CART 算法是一套完整的包含上述三个环节的算法。

</font>