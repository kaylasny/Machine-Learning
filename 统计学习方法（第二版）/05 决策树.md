# 决策树

> **定义 5.1（决策树）**  
> 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

假设给定训练数据集：

$$
D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}
$$

其中，$x_i = (x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})^T$ 为输入实例（特征向量），$n$ 为特征个数，$y_i \in \{1, 2, \cdots, K\}$ 为类标记，$i = 1, 2, \cdots, N$，$N$ 为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

- 决策树学习用损失函数表示这一目标，决策树学习的策略是**以损失函数为目标函数的最小化**。

- 决策树学习通常包括三个步骤：<strong style="color:red">特征选取、决策树的生成和决策树的修剪</strong> 。

- 决策树学习本质上是从训练数据集中归纳出一组分类规则，希望获得一个与训练数据矛盾较小同时具有很好的泛化能力。

- 针对过拟合现象，需要对已经生成的树自下而上进行剪枝，让树变得简单，从而使它有更好的泛化能力。

- 如果特征数量很多，可以在决策树学习开始的时候对特征进行选择，只留下对训练数据有足够分类能力的特征。



## 特征选取

<strong style="color:red">信息增益</strong>

> **定义 5.2 （信息增益）**  
> 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即：
>
> $$
> g(D, A) = H(D) - H(D|A)
> $$



> **算法 5.1 （信息增益的算法）**  
> 输入：训练数据集 $D$ 和特征 $A$；  
> 输出：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$。  
> 
> 1. **计算数据集 $D$ 的经验熵 $H(D)$**  
> 
>    $$
>    H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}
>    $$
> 
> 2. **计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$**  
> 
>    $$
>    H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}
>    $$
> 
> 3. **计算信息增益**  
> 
>    $$
>    g(D, A) = H(D) - H(D|A)
>    $$

<strong style="color:red">信息增益比</strong>

> **定义 5.3 （信息增益比）**  
> 特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A)$ 定义为其信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即：
> 
> $$
> g_R(D, A) = \frac{g(D, A)}{H_A(D)} 
> $$
> 
> 其中，
> 
> $$
> H_A(D) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
> $$
> 
> $n$ 是特征 $A$ 取值的个数。



## 决策树的生成

<strong style="color:red">ID3 算法</strong>

> **算法 5.2 （ID3 算法）**  
> 输入：训练数据集 $D$，特征集 $A$ 阈值 $\varepsilon$；  
> 输出：决策树 $T$。  
> 
> 1. 若 $D$ 中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 2. 若 $A = \emptyset$，则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 3. 否则，按算法 5.1 计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_g$；  
> 4. 如果 $A_g$ 的信息增益小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；  
> 6. 对第 $i$ 个子结点，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归地调用步骤 (1)～(5)，得到子树 $T_i$，返回 $T_i$。

ID3 算法中只有树的生成，容易导致过拟合。



<strong style="color:red">C4.5 算法</strong>

> **算法 5.3 （C4.5 的生成算法）**  
> 输入：训练数据集 $D$，特征集 $A$ 阈值 $\varepsilon$；  
> 输出：决策树 $T$。  
>
> 1. 如果 $D$ 中所有实例属于同一类 $C_k$，则置 $T$ 为单结点树，并将 $C_k$ 作为该结点的类，返回 $T$；  
> 2. 如果 $A = \emptyset$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；  
> 3. 否则，按式 (5.10) 计算 $A$ 中各特征对 $D$ 的信息增益比，选择信息增益比最大的特征 $A_g$；  
> 4. 如果 $A_g$ 的信息增益比小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；  
> 5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；  
> 6. 对结点 $i$，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归地调用步骤 (1)～(5)，得到子树 $T_i$，返回 $T_i$。

C4.5 算法与 ID3 算法相似，在生成过程中使用信息增益比来选择特征。



## 决策树的剪枝

设树 $T$ 的叶结点个数为 $|T|$，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $N_{tk}$ 个，$k=1, 2, \cdots, K$，$H_t(T)$ 为叶结点 $t$ 上的经验熵，$\alpha \geq 0$ 为参数，则决策树学习的损失函数可以定义为：

$$
C_\alpha(T) = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T|
\tag{5.11}
$$

其中经验熵为：

$$
H_t(T) = -\sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}
\tag{5.12}
$$

在损失函数中，将式 (5.11) 右端的第 1 项记作：

$$
C(T) = \sum_{t=1}^{|T|} N_t H_t(T) = -\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{tk} \log \frac{N_{tk}}{N_t}
\tag{5.13}
$$

这时有：

$$
C_\alpha(T) = C(T) + \alpha |T|
\tag{5.14}
$$

式 (5.14) 中，$C(T)$ 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$ 表示模型复杂度，参数 $\alpha \geq 0$ 控制两者之间的影响。较大的 $\alpha$ 倾向于选择较简单的模型（树），较小的 $\alpha$ 倾向于选择较复杂的模型（树）。$\alpha=0$ 意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

剪枝，就是当 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 $\alpha$ 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。

> **算法 5.4（树的剪枝算法）**  
> 输入：生成算法产生的整个树 $T$，参数 $\alpha$；  
> 输出：修剪后的子树 $T_\alpha$。
> 1. 计算每个结点的经验熵。  
> 2. 递归地从树的叶结点向上回缩。  
>    - 设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_B$ 与 $T_A$，其对应的损失函数值分别是 $C_\alpha(T_B)$ 与 $C_\alpha(T_A)$，如果
>      $$
>      C_\alpha(T_A) \leq C_\alpha(T_B)
>      $$
>      则进行剪枝，即将父结点变为新的叶结点。
> 3. 返回 (2)，直至不能继续为止，得到损失函数最小的子树 $T_\alpha$。



## CART 算法

