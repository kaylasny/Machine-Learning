# 决策树

> **定义 5.1（决策树）**  
> 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

假设给定训练数据集：

$$
D = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}
$$

其中，$x_i = (x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})^T$ 为输入实例（特征向量），$n$ 为特征个数，$y_i \in \{1, 2, \cdots, K\}$ 为类标记，$i = 1, 2, \cdots, N$，$N$ 为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

- 决策树学习用损失函数表示这一目标，决策树学习的策略是**以损失函数为目标函数的最小化**。

- 决策树学习通常包括三个步骤：<strong style="color:red">特征选取、决策树的生成和决策树的修剪</strong> 。

- 决策树学习本质上是从训练数据集中归纳出一组分类规则，希望获得一个与训练数据矛盾较小同时具有很好的泛化能力。

- 针对过拟合现象，需要对已经生成的树自下而上进行剪枝，让树变得简单，从而使它有更好的泛化能力。

- 如果特征数量很多，可以在决策树学习开始的时候对特征进行选择，只留下对训练数据有足够分类能力的特征。



## 特征选取

> **定义 5.2 （信息增益）**  
> 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即：
>
> $$
> g(D, A) = H(D) - H(D|A)
> $$



> **算法 5.1 （信息增益的算法）**  
> 输入：训练数据集 $D$ 和特征 $A$；  
> 输出：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$。  
> 
> 1. **计算数据集 $D$ 的经验熵 $H(D)$**  
> 
>    $$
>    H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}
>    $$
> 
> 2. **计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$**  
> 
>    $$
>    H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}
>    $$
> 
> 3. **计算信息增益**  
> 
>    $$
>    g(D, A) = H(D) - H(D|A)
>    $$



> **定义 5.3 （信息增益比）**  
> 特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A)$ 定义为其信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即：
> 
> $$
> g_R(D, A) = \frac{g(D, A)}{H_A(D)} 
> $$
> 
> 其中，
> 
> $$
> H_A(D) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
> $$
> 
> $n$ 是特征 $A$ 取值的个数。



## 决策树的生成

<strong style="color:red">ID3 算法</strong>

> **算法 5.2 （ID3 算法）**  
> 输入：训练数据集 $D$，特征集 $A$ 阈值 $\varepsilon$；  
> 输出：决策树 $T$。  
> 
> 1. 若 $D$ 中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 2. 若 $A = \emptyset$，则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 3. 否则，按算法 5.1 计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_g$；  
> 4. 如果 $A_g$ 的信息增益小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；  
> 5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；  
> 6. 对第 $i$ 个子结点，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归地调用步骤 (1)～(5)，得到子树 $T_i$，返回 $T_i$。

ID3 算法中只有树的生成，容易导致过拟合。



<strong style="color:red">C4.5 算法</strong>

> **算法 5.3 （C4.5 的生成算法）**  
> 输入：训练数据集 $D$，特征集 $A$ 阈值 $\varepsilon$；  
> 输出：决策树 $T$。  
>
> 1. 如果 $D$ 中所有实例属于同一类 $C_k$，则置 $T$ 为单结点树，并将 $C_k$ 作为该结点的类，返回 $T$；  
> 2. 如果 $A = \emptyset$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；  
> 3. 否则，按式 (5.10) 计算 $A$ 中各特征对 $D$ 的信息增益比，选择信息增益比最大的特征 $A_g$；  
> 4. 如果 $A_g$ 的信息增益比小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；  
> 5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；  
> 6. 对结点 $i$，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归地调用步骤 (1)～(5)，得到子树 $T_i$，返回 $T_i$。

C4.5 算法与 ID3 算法相似，在生成过程中使用信息增益比来选择特征。



## 决策树的剪枝



